# 上市公司年报词频统计

王帅龙 2017012265

## 词频项目优化建议

- 提升在线pdf的转文本速度

- 正则匹配清除前100的高频词，可有效压缩文本空间，降低jieba分词的运行负荷

## 当前版本代码

### 一、从巨潮资讯网站获取文本（在线转文本，多进程实现）

```python
import requests
import random
import logging
import time
from concurrent.futures import ProcessPoolExecutor
from urllib.request import urlopen
from urllib.request import Request
from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LTTextBoxHorizontal, LAParams
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfinterp import PDFTextExtractionNotAllowed
from pdfminer.pdfparser import PDFParser, PDFDocument


User_Agent = [
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR "
    "3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
    "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; "
    ".NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
    "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR "
    "3.0.04506.30)",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 ("
    "Change: 287 c9dfb30)",
    "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0"
]  # User_Agent的集合

headers = {'Accept': 'application/json, text/javascript, */*; q=0.01',
           "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
           "Accept-Encoding": "gzip, deflate",
           "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-HK;q=0.6,zh-TW;q=0.5",
           'Host': 'www.cninfo.com.cn',
           'Origin': 'http://www.cninfo.com.cn',
           'Referer': 'http://www.cninfo.com.cn/new/commonUrl?url=disclosure/list/notice',
           'X-Requested-With': 'XMLHttpRequest'
           }


# 在线pdf转txt并保存到本地
def online_pdf_transformation(_path, txt_name):
    t1 = time.time()
    logging.propagate = False
    logging.getLogger().setLevel(logging.ERROR)
    request = Request(url=_path, headers={'User-Agent': random.choice(User_Agent)})
    fp = urlopen(request)
    parser_pdf = PDFParser(fp)
    doc = PDFDocument()
    parser_pdf.set_document(doc)
    doc.set_parser(parser_pdf)
    doc.initialize()

    if not doc.is_extractable:
        raise PDFTextExtractionNotAllowed
    else:
        rsrcmgr = PDFResourceManager()
        laparams = LAParams()
        device = PDFPageAggregator(rsrcmgr, laparams=laparams)
        interpreter = PDFPageInterpreter(rsrcmgr, device)
        page_finished = 0
        txt_filepath = 'C://Users//hp//Desktop//股吧词频库//巨潮资讯网//' + txt_name
        with open(txt_filepath, 'w', encoding='utf-8') as fw:
            print("num page:{}".format(len(list(doc.get_pages()))))
            for page in doc.get_pages():
                interpreter.process_page(page)
                layout = device.get_result()
                page_finished += 1
                print("Transforming...", page_finished, " page")
                for out in layout:
                    if isinstance(out, LTTextBoxHorizontal):
                        results = out.get_text()
                        fw.write(results)
    print('用时', time.time() - t1, 'sec')


# post请求巨潮信息网ajax加载的json数据
def http_post_ajax(one_page):
    query_path = 'http://www.cninfo.com.cn/new/hisAnnouncement/query'
    headers['User-Agent'] = random.choice(User_Agent)  # 定义User_Agent
    query = {'pageNum': one_page,  # 页码
             'pageSize': 30,
             'tabName': 'fulltext',
             'column': 'szse',
             'stock': '',
             'searchkey': '',
             'secid': '',
             'plate': 'sh',  # 自定义爬取哪个板块，上证还是深证，主板中小板还是创业板
             'category': 'category_ndbg_szsh;',  # 年度报告
             'trade': '',
             'seDate': '2019-02-01+~+2020-02-01'  # 时间区间
             }
    namelist = requests.post(query_path, headers=headers, data=query)
    print('正在获取第', one_page, '页信息')
    return namelist.json()['announcements']  # json中的年度报告信息


# 单线程解析json数据，得到在线pdf的网址链接
def saving_to_desk(single_page):  # 在线解析年报，保存txt文本到本地
    download_path = 'http://static.cninfo.com.cn/'
    for i in single_page:
        if i['announcementTitle'] == '2018年年度报告（更新后）' or i['announcementTitle'] == '2018年年度报告':
            download = download_path + i["adjunctUrl"]
            name = i["secCode"] + '_' + i['secName'] + '_' + i['announcementTitle'] + '.txt'
            if '*' in name:
                name = name.replace('*', '')
            print('正在保存...', name)
            online_pdf_transformation(download, name)
        else:
            continue


# 多进程解析json数据，得到在线pdf的网址链接，2倍速
def multi_process_saving(single_page):
    t0 = time.time()
    tasks = []
    with ProcessPoolExecutor(max_workers=5) as executor:
        for i in single_page:
            if i['announcementTitle'] == '2018年年度报告（更新后）' or i['announcementTitle'] == '2018年年度报告':
                download = 'http://static.cninfo.com.cn/' + i["adjunctUrl"]
                name = i["secCode"] + '_' + i['secName'] + '_' + i['announcementTitle'] + '.txt'
                if '*' in name:
                    name = name.replace('*', '')
                print('正在保存...', name)
                result = executor.submit(online_pdf_transformation, download, name)
                tasks.append(result)
            else:
                continue
    print('解析该页pdf文件总耗时：', time.time() - t0, 'sec')


# 循环爬取巨潮信息的搜索页面
if __name__ == '__main__':
    for page in range(46, 50):
        multi_process_saving(http_post_ajax(page))

```



### 二、对得到的文本做词频统计（保存词云图）

```python
import os  # 系统文件库
import re  # 正则表达式库
import collections  # 词频统计库
import numpy as np  # numpy数据处理库
import pandas
import jieba  # 结巴分词
import wordcloud  # 词云展示库
from PIL import Image  # 图像处理库
import matplotlib.pyplot as plt  # 图像展示库


# 词频展示
def jpg_show(class_counts, file_name):
    mask = np.array(Image.open('C://Users//hp//Desktop//股吧词频库//背景图片库//china_in_red.png'))  # 定义词频背景
    wc = wordcloud.WordCloud(
        scale=8,  # 设置图片清晰度
        font_path='C:/Windows/Fonts/simhei.ttf',  # 设置字体格式
        # mask=mask,  # 设置背景图
        max_words=75,  # 最多显示词数
        max_font_size=100,  # 字体最大值
        background_color="white"  # 背景颜色为白色
    )
    jpg_path = 'C://Users//hp//Desktop//股吧词频库//词云图库//上证//' + file_name + '.jpg'

    # 从字典生成词云
    wc.generate_from_frequencies(class_counts)
    # image_colors = wordcloud.ImageColorGenerator(mask)  # 从背景图建立颜色方案
    # wc.recolor(color_func=image_colors)  # 将词云颜色设置为背景图方案
    wc.to_file(jpg_path)
    # wc.to_file('wordcloud.jpg')

    # 展示词云
    # plt.imshow(wc)  # 显示词云
    # plt.axis('off')  # 关闭坐标轴
    # plt.show()  # 显示图像


# 词频统计并保存csv文件
def words_stats(file_name):
    txt_path = 'C://Users//hp//Desktop//股吧词频库//巨潮资讯网//上证//' + file_name + '.txt'
    csv_path = 'C://Users//hp//Desktop//股吧词频库//词频统计库//上证//' + file_name + '.csv'
    fn = open(txt_path, 'r', encoding='utf-8')  # 打开文件
    string_data = fn.read()  # 读出整个文件
    fn.close()  # 关闭文件

    # 文本预处理：定义正则表达式匹配模式
    pattern = re.compile(u"[^\u4e00-\u9fa5]+")
    string_data = re.sub(pattern, '', string_data)  # 保留中文字符
    print(string_data)
    with open(txt_path, 'w', encoding='utf-8') as fw:
        fw.write(string_data)
    fw.close()

    # 文本分词
    seg_list_exact = jieba.cut(string_data, cut_all=False)  # 精确模式分词，隐马尔科夫模型

    # 读取停用词表
    object_list = []
    stop_words = [line.strip() for line in
                  open('C://Users//hp//Desktop//股吧词频库//chineseStopWords.txt', 'r',
                       encoding='utf-8-sig').readlines()]

    for word in seg_list_exact:  # 循环读出每个分词
        if word not in stop_words:  # 如果不在去除词库中
            object_list.append(word)  # 分词追加到列表

    # 词频统计
    word_counts = collections.Counter(object_list)  # 对分词做词频统计，word_counts为count类对象，可做词云图
    print(len(word_counts))
    word_counts_all = word_counts.most_common(len(word_counts))  # 获取全部分词
    print(word_counts_all)  # 输出检查

    # word_counts写入csv文件
    save = pandas.DataFrame(list(word_counts_all), columns=['分词排序', '词频'])
    try:
        save.to_csv(csv_path)
    except UnicodeEncodeError:
        print("编码错误, 该数据无法写到文件中, 直接忽略该数据")

    # 制作词云图
    jpg_show(word_counts, file_name)


if __name__ == '__main__':
    dir_path = 'C://Users//hp//Desktop//股吧词频库//巨潮资讯网//上证'
    print(os.listdir(dir_path))
    for each_txt in os.listdir(dir_path):
        each_name = each_txt.split('.')[0]
        words_stats(each_name)

```

